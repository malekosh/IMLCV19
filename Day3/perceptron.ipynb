{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from sklearn.datasets import fetch_openml\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: (70000, 784) , labels: (70000,)\n"
     ]
    }
   ],
   "source": [
    "data_path = os.path.join(os.path.dirname(os.getcwd()), 'datasets')\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True, data_home=data_path)\n",
    "print('data:', X.shape, ',', 'labels:', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data: (200, 784) , train labels: (200,)\n",
      "val data: (10, 784) , val labels: (10,)\n",
      "test data: (10, 784) , test labels: (10,)\n"
     ]
    }
   ],
   "source": [
    "unique_labels = np.unique(y)\n",
    "num_classes = len(unique_labels)\n",
    "\n",
    "n_train_pc = 20 # number of train samples per class\n",
    "n_val_pc = 1 # number of validation samples per class\n",
    "n_test_pc = 1 # number of test samples per class\n",
    "\n",
    "n_train = n_train_pc * num_classes\n",
    "n_val = n_val_pc * num_classes\n",
    "n_test = n_test_pc * num_classes\n",
    "\n",
    "train_data = np.zeros((num_classes,n_train_pc,784))\n",
    "train_labels = np.zeros((num_classes,n_train_pc))\n",
    "\n",
    "val_data = np.zeros((num_classes,n_val_pc,784))\n",
    "val_labels = np.zeros((num_classes,n_val_pc))\n",
    "\n",
    "test_data = np.zeros((num_classes,n_test_pc,784))\n",
    "test_labels = np.zeros((num_classes,n_test_pc))\n",
    "\n",
    "for l_idx, l in enumerate(unique_labels):\n",
    "    idxs = np.squeeze(np.argwhere(y == l))\n",
    "    idxs = np.random.choice(idxs, n_train_pc + n_val_pc + n_test_pc, replace=False)\n",
    "    \n",
    "    train_data[l_idx] = X[idxs[:n_train_pc]]\n",
    "    train_labels[l_idx] = y[idxs[:n_train_pc]]\n",
    "    \n",
    "    val_data[l_idx] = X[idxs[n_train_pc:n_train_pc + n_val_pc]]\n",
    "    val_labels[l_idx] = y[idxs[n_train_pc:n_train_pc + n_val_pc]]\n",
    "    \n",
    "    test_data[l_idx] = X[idxs[n_train_pc + n_val_pc:]]\n",
    "    test_labels[l_idx] = y[idxs[n_train_pc + n_val_pc:]]\n",
    "    \n",
    "# ravel the data ---------------------------------------------------------------\n",
    "train_data = train_data.reshape(-1,784)\n",
    "train_labels = np.ravel(train_labels).astype(np.int)\n",
    "\n",
    "val_data = val_data.reshape(-1,784)\n",
    "val_labels = np.ravel(val_labels).astype(np.int)\n",
    "\n",
    "test_data = test_data.reshape(-1,784)\n",
    "test_labels = np.ravel(test_labels).astype(np.int)\n",
    "\n",
    "# shuffle the data -------------------------------------------------------------\n",
    "train_idxs = np.arange(len(train_data))\n",
    "_ = np.random.shuffle(train_idxs)\n",
    "\n",
    "train_data = train_data[train_idxs]\n",
    "train_labels = train_labels[train_idxs]\n",
    "\n",
    "#-----------------------------------------\n",
    "\n",
    "val_idxs = np.arange(len(val_data))\n",
    "_ = np.random.shuffle(val_idxs)\n",
    "\n",
    "val_data = val_data[val_idxs]\n",
    "val_labels = val_labels[val_idxs]\n",
    "\n",
    "#-----------------------------------------\n",
    "\n",
    "test_idxs = np.arange(len(test_data))\n",
    "_ = np.random.shuffle(test_idxs)\n",
    "\n",
    "test_data = test_data[test_idxs]\n",
    "test_labels = test_labels[test_idxs]\n",
    "\n",
    "#-----------------------------------------\n",
    "\n",
    "print('train data:', train_data.shape, ',', 'train labels:', train_labels.shape)\n",
    "print('val data:', val_data.shape, ',', 'val labels:', val_labels.shape)\n",
    "print('test data:', test_data.shape, ',', 'test labels:', test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_two_layer_perceptron(X, y, X_val, y_val,\n",
    "                               hidden_size, batch_size, \n",
    "                               num_epochs, learning_rate, \n",
    "                               learning_rate_decay, reg_factor):\n",
    "    \n",
    "    '''\n",
    "    Inputs:\n",
    "        X: train data (N,m)\n",
    "        y: train labels (N,)\n",
    "        hidden_size: number of neurons in the hidden layer (h,)\n",
    "        batch_size: number of samples per batch\n",
    "        num_epochs: number of total epochs\n",
    "    Returns:\n",
    "    \n",
    "    '''\n",
    "\n",
    "    num_train = X.shape[0] # number of train samples\n",
    "    num_batches = num_train // batch_size # number of batches\n",
    "    \n",
    "    input_size = X.shape[1] # number of features in the train data\n",
    "    output_size = len(np.unique(y)) # number of classes\n",
    "    \n",
    "    # Initialize network parameters\n",
    "    params = {}\n",
    "    std = 1e-4 # standard deviation\n",
    "    params['W1'] = std * np.random.randn(input_size, hidden_size) # (m,h)\n",
    "    params['b1'] = np.zeros(hidden_size) # (h,)\n",
    "    params['W2'] = std * np.random.randn(hidden_size, output_size) # (h,c)\n",
    "    params['b2'] = np.zeros(output_size) # (c,)\n",
    "    \n",
    "    \n",
    "    loss_hisory = []\n",
    "    train_acc_history = []\n",
    "    val_acc_history = []\n",
    "    for e in range(num_epochs):  \n",
    "        \n",
    "        # Evaluate one epoch -----------------------------------------------------------------------------\n",
    "        train_acc = (predict_two_layer_perceptron(X_batch) == y_batch).mean()\n",
    "        val_acc = (predict_two_layer_perceptron(X_val) == y_val).mean()\n",
    "        \n",
    "        train_acc_history.append(train_acc)\n",
    "        val_acc_history.append(val_acc)\n",
    "        \n",
    "        print('Evaluate epoch:', e, 'out of:', num_epochs);\n",
    "        print('train accuracy:', train_acc)\n",
    "        print('validation accuracy:\\n', val_acc)\n",
    "        \n",
    "        # Shuffle train data ----------------------------------------------------------------------------\n",
    "        idxs = np.arange(X.shape[0])\n",
    "        _ = np.random.shuffle(idxs)\n",
    "        X = X[idxs]\n",
    "        y = y[idxs]\n",
    "        \n",
    "        # Decay learning rate ---------------------------------------------------------------------------\n",
    "        learning_rate *= learning_rate_decay\n",
    "\n",
    "        # Train one epoch -------------------------------------------------------------------------------\n",
    "        print('Train Epoch:', e, 'out of:', num_epochs)\n",
    "        for b in range(num_batches):\n",
    "            \n",
    "            # Extract current batch\n",
    "            X_batch = X[b*batch_size : (b+1)*batch_size]\n",
    "            y_batch = y[b*batch_size : (b+1)*batch_size]\n",
    "            \n",
    "            # One forward pass and one backward pass through the whole network\n",
    "            grads, loss = run_perceptron(params, X_batch, y_batch, reg_factor)\n",
    "            loss_history.append(loss)\n",
    "            \n",
    "            # Update network's parameters\n",
    "            params['W1'] -= learning_rate * grads['W1']\n",
    "            params['b1'] -= learning_rate * grads['b1']\n",
    "            params['W2'] -= learning_rate * grads['W2']\n",
    "            params['b2'] -= learning_rate * grads['b2']\n",
    "            \n",
    "            if (b+1) % 100 == 0:\n",
    "                print('Batch number:', b, 'out of:', num_batches)\n",
    "                print('loss:\\n', loss)\n",
    "                \n",
    "    history = (loss_history, train_acc_history, val_acc_history)\n",
    "    \n",
    "    return params, history\n",
    "            \n",
    "\n",
    "def run_perceptron(params, X, y=None, reg_factor=0.0)\n",
    "    N, m = X.shape\n",
    "\n",
    "    # Unpack the parameters\n",
    "    W1, b1 = params['W1'], params['b1']\n",
    "    W2, b2 = params['W2'], params['b2']\n",
    "    \n",
    "    # Forward pass --------------------------------------------------------------------------------------\n",
    "    # Compute hidden scores and apply ReLU activation function\n",
    "    H = np.dot(X, W1) + b1 # (N,h)\n",
    "    H = np.maximum(0, H) # apply ReLU\n",
    "    \n",
    "    # Compute output scores\n",
    "    scores = np.dot(H, W2) + b2 #(N,c)\n",
    "    \n",
    "    # Compute the softmax cross-entropy loss + L2 regularization for W1 and W2\n",
    "    # Softmax function\n",
    "    exp_scores = np.exp(scores)\n",
    "    correct_scores = exp_scores[range(N),y]\n",
    "    correct_probs = correct_scores / np.sum(exp_scores, axis=1) # (N,)\n",
    "    \n",
    "    # Cross-entropy loss \n",
    "    loss = -np.log(correct_probs) # (N,)\n",
    "    loss = np.mean(loss) # average over all data samples\n",
    "    \n",
    "    # L2 regularization\n",
    "    reg = reg_factor*(np.sum(W1**2) + np.sum(W2**2))\n",
    "    \n",
    "    total_loss == loss + reg\n",
    "    \n",
    "    # If the network is not being trained, just apply forward pass\n",
    "    if y is None:\n",
    "        return scores, total_loss\n",
    "    \n",
    "    \n",
    "    # Backward pass -------------------------------------------------------------------------------------\n",
    "    # Partial-derivative of total loss w.r.t scores\n",
    "    dscores = np.exp(scores) / np.sum(scores, axis=1, keepdims=True)\n",
    "    dscores[range(N),y] -= 1\n",
    "    dscores /= N # (N,c)\n",
    "    \n",
    "    # Partial-derivative of total loss with respect to hidden layer\n",
    "    dH = np.dot(dscores, W2.T) # (N,h)\n",
    "    dH[H<0] = 0 # back-propagate only to the values that contributed in the forward pass \n",
    "    \n",
    "    # Calculate gradients \n",
    "    gards = {}\n",
    "    grads['W1'] = np.dot(X.T, dH) + reg_factor*w1\n",
    "    grads['b1'] = np.sum(dH, axis=0)\n",
    "    grads['W2'] = np.dot(H.T, dscores) + reg_factor*w2\n",
    "    grads['b2'] = np.sum(dscores, axis=0)\n",
    "    \n",
    "    return grads, loss\n",
    "\n",
    "\n",
    "def evaluate(X, y, params):\n",
    "    pred_scores, pred_loss = run_perceptron(params, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
