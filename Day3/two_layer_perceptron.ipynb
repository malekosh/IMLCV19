{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from sklearn.datasets import fetch_openml\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(os.path.dirname(os.getcwd()), 'datasets')\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True, data_home=data_path)\n",
    "print('data:', X.shape, ',', 'labels:', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Validation-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unique_labels = np.unique(y)\n",
    "chosen_labels = unique_labels[:]\n",
    "num_classes = len(chosen_labels)\n",
    "\n",
    "n_train_pc = 5000 # number of train samples per class\n",
    "n_val_pc = 500 # number of validation samples per class\n",
    "n_test_pc = 500 # number of test samples per class\n",
    "\n",
    "n_train = n_train_pc * num_classes\n",
    "n_val = n_val_pc * num_classes\n",
    "n_test = n_test_pc * num_classes\n",
    "\n",
    "train_data = np.zeros((num_classes,n_train_pc,784))\n",
    "train_labels = np.zeros((num_classes,n_train_pc))\n",
    "\n",
    "val_data = np.zeros((num_classes,n_val_pc,784))\n",
    "val_labels = np.zeros((num_classes,n_val_pc))\n",
    "\n",
    "test_data = np.zeros((num_classes,n_test_pc,784))\n",
    "test_labels = np.zeros((num_classes,n_test_pc))\n",
    "\n",
    "# split the data -------------------------------------------------------------------\n",
    "for l_idx, l in enumerate(chosen_labels):\n",
    "    idxs = np.squeeze(np.argwhere(y == l))\n",
    "    idxs = np.random.choice(idxs, n_train_pc + n_val_pc + n_test_pc, replace=False)\n",
    "    \n",
    "    train_data[l_idx] = X[idxs[:n_train_pc]]\n",
    "    train_labels[l_idx] = y[idxs[:n_train_pc]]\n",
    "    \n",
    "    val_data[l_idx] = X[idxs[n_train_pc:n_train_pc + n_val_pc]]\n",
    "    val_labels[l_idx] = y[idxs[n_train_pc:n_train_pc + n_val_pc]]\n",
    "    \n",
    "    test_data[l_idx] = X[idxs[n_train_pc + n_val_pc:]]\n",
    "    test_labels[l_idx] = y[idxs[n_train_pc + n_val_pc:]]\n",
    "    \n",
    "# ravel the data ---------------------------------------------------------------\n",
    "train_data = train_data.reshape(-1,784)\n",
    "train_labels = np.ravel(train_labels).astype(np.int)\n",
    "\n",
    "val_data = val_data.reshape(-1,784)\n",
    "val_labels = np.ravel(val_labels).astype(np.int)\n",
    "\n",
    "test_data = test_data.reshape(-1,784)\n",
    "test_labels = np.ravel(test_labels).astype(np.int)\n",
    "\n",
    "# shuffle the data -------------------------------------------------------------\n",
    "train_idxs = np.arange(len(train_data))\n",
    "_ = np.random.shuffle(train_idxs)\n",
    "\n",
    "train_data = train_data[train_idxs]\n",
    "train_labels = train_labels[train_idxs]\n",
    "\n",
    "#-----------------------------------------\n",
    "\n",
    "val_idxs = np.arange(len(val_data))\n",
    "_ = np.random.shuffle(val_idxs)\n",
    "\n",
    "val_data = val_data[val_idxs]\n",
    "val_labels = val_labels[val_idxs]\n",
    "\n",
    "#-----------------------------------------\n",
    "\n",
    "test_idxs = np.arange(len(test_data))\n",
    "_ = np.random.shuffle(test_idxs)\n",
    "\n",
    "test_data = test_data[test_idxs]\n",
    "test_labels = test_labels[test_idxs]\n",
    "\n",
    "#-----------------------------------------\n",
    "\n",
    "print('train data:', train_data.shape, ',', 'train labels:', train_labels.shape)\n",
    "print('val data:', val_data.shape, ',', 'val labels:', val_labels.shape)\n",
    "print('test data:', test_data.shape, ',', 'test labels:', test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_image = np.mean(train_data, axis=0)\n",
    "train_data -= mean_image\n",
    "val_data -= mean_image\n",
    "test_data -= mean_image\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_two_layer_perceptron(X, y, X_val, y_val,\n",
    "                               hidden_size, batch_size, \n",
    "                               num_epochs, learning_rate, \n",
    "                               learning_rate_decay, reg_factor):\n",
    "    \n",
    "    '''\n",
    "    Inputs:\n",
    "        X: train data (N,m)\n",
    "        y: train labels (N,)\n",
    "        X_val: validation data (N2,m)\n",
    "        y_val: validation labels (N2,)\n",
    "        hidden_size: number of neurons in the hidden layer (h,)\n",
    "        batch_size: number of samples per batch\n",
    "        num_epochs: number of total epochs\n",
    "    Returns:\n",
    "        params: a dictionary containing the model's paramteres after training\n",
    "        hist: a dictionary containing the train history\n",
    "    '''\n",
    "\n",
    "    num_train = X.shape[0] # number of train samples\n",
    "    num_batches = num_train // batch_size # number of batches\n",
    "    \n",
    "    input_size = X.shape[1] # number of features in the train data\n",
    "    output_size = len(np.unique(y)) # number of classes\n",
    "    \n",
    "    # Initialize network parameters\n",
    "    params = {}\n",
    "    #np.random.seed(0)\n",
    "    std = 1e-6 # standard deviation\n",
    "    params['W1'] = std * np.random.randn(input_size, hidden_size) # (m,h)\n",
    "    params['b1'] = np.zeros(hidden_size) # (h,)\n",
    "    params['W2'] = std * np.random.randn(hidden_size, output_size) # (h,c)\n",
    "    params['b2'] = np.zeros(output_size) # (c,)\n",
    "    \n",
    "    loss_hist = []\n",
    "    train_acc_hist = []\n",
    "    val_acc_hist = []\n",
    "    for e in range(num_epochs):  \n",
    "        \n",
    "        # Shuffle train data ----------------------------------------------------------------------------\n",
    "        idxs = np.arange(X.shape[0])\n",
    "        _ = np.random.shuffle(idxs)\n",
    "        X = X[idxs]\n",
    "        y = y[idxs]\n",
    "        \n",
    "        # Decay learning rate ---------------------------------------------------------------------------\n",
    "        learning_rate *= learning_rate_decay\n",
    "\n",
    "        # Train one epoch -------------------------------------------------------------------------------\n",
    "        print('Train Epoch:', e+1, 'out of', num_epochs)\n",
    "        print('------------------------------------------------------------------------------------------')\n",
    "\n",
    "        for b in range(num_batches):\n",
    "            \n",
    "            # Extract current batch\n",
    "            X_batch = X[b*batch_size : (b+1)*batch_size]\n",
    "            y_batch = y[b*batch_size : (b+1)*batch_size]\n",
    "            \n",
    "            # One forward pass and one backward pass through the whole network\n",
    "            grads, loss = run_perceptron(params, X_batch, y_batch, reg_factor)\n",
    "            loss_hist.append(loss)\n",
    "            \n",
    "            # Update network's parameters\n",
    "            params['W1'] -= learning_rate * grads['W1']\n",
    "            params['b1'] -= learning_rate * grads['b1']\n",
    "            params['W2'] -= learning_rate * grads['W2']\n",
    "            params['b2'] -= learning_rate * grads['b2']\n",
    "                        \n",
    "            # Check and store train accuracy on the current batch\n",
    "            train_acc = np.mean(predict_two_layer_perceptron(params, X_batch) == y_batch)\n",
    "            train_acc_hist.append(train_acc)\n",
    "            \n",
    "            if (b+1) % 100 == 0:\n",
    "                print('Batch number:', b+1, 'out of', num_batches)\n",
    "                print('loss:', loss)\n",
    "                print('train accuracy:', train_acc, '\\n')\n",
    "                \n",
    "\n",
    "        # Evaluate one epoch -----------------------------------------------------------------------------\n",
    "        val_acc = np.mean(predict_two_layer_perceptron(params, X_val) == y_val)        \n",
    "        val_acc_hist.append(val_acc)\n",
    "        \n",
    "        print('------------------------------------------------------------------------------------------')\n",
    "        print('Evaluate epoch:', e+1, 'out of', num_epochs);\n",
    "        print('validation accuracy:', val_acc, '\\n')\n",
    "        \n",
    "    hist = {}\n",
    "    hist['loss'] = loss_hist\n",
    "    hist['train_acc'] = train_acc_hist\n",
    "    hist['val_acc'] = val_acc_hist\n",
    "    \n",
    "    return params, hist\n",
    "            \n",
    "\n",
    "def run_perceptron(params, X, y=None, reg_factor=0.0):\n",
    "    N, m = X.shape\n",
    "\n",
    "    # Unpack the parameters\n",
    "    W1, b1 = params['W1'], params['b1']\n",
    "    W2, b2 = params['W2'], params['b2']\n",
    "    \n",
    "    # Forward pass --------------------------------------------------------------------------------------\n",
    "    # Compute hidden scores and apply ReLU activation function\n",
    "    H = np.dot(X, W1) + b1 # (N,h)\n",
    "    H = np.maximum(0, H) # apply ReLU\n",
    "    \n",
    "    # Compute output scores\n",
    "    scores = np.dot(H, W2) + b2 #(N,c)\n",
    "    \n",
    "    # If the network is not being trained, just apply forward pass\n",
    "    if y is None:\n",
    "        return scores\n",
    "    \n",
    "    # Compute the softmax cross-entropy loss + L2 regularization for W1 and W2 ---------------------------\n",
    "    # Softmax function\n",
    "    exp_scores = np.exp(scores - np.max(scores, axis=1, keepdims=True))\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "        \n",
    "    # Cross-entropy loss \n",
    "    correct_probs = probs[range(N),y]\n",
    "    loss = -np.log(correct_probs) # (N,)\n",
    "    loss = np.mean(loss) # average over all data samples\n",
    "    \n",
    "    # L2 regularization\n",
    "    reg = 0.5*reg_factor*(np.sum(W1**2) + np.sum(W2**2))\n",
    "    \n",
    "    total_loss = loss + reg\n",
    "    \n",
    "    # Backward pass -------------------------------------------------------------------------------------\n",
    "    # Partial-derivative of total loss w.r.t scores\n",
    "    dscores = probs.copy()\n",
    "    dscores[range(N),y] -= 1\n",
    "    dscores /= N # (N,c)\n",
    "    \n",
    "    # Partial-derivative of total loss with respect to hidden layer\n",
    "    dH = np.dot(dscores, W2.T) # (N,h)\n",
    "    dH[H<0] = 0 # back-propagate only to the values that contributed in the forward pass \n",
    "    \n",
    "    # Calculate gradients \n",
    "    grads = {}\n",
    "    grads['W1'] = np.dot(X.T, dH) + reg_factor*W1\n",
    "    grads['b1'] = np.sum(dH, axis=0)\n",
    "    grads['W2'] = np.dot(H.T, dscores) + reg_factor*W2\n",
    "    grads['b2'] = np.sum(dscores, axis=0)\n",
    "    \n",
    "    return grads, total_loss\n",
    "\n",
    "\n",
    "def predict_two_layer_perceptron(params, X, y=None):\n",
    "    pred_scores = run_perceptron(params, X)\n",
    "    y_pred = np.argmax(pred_scores, axis=1)\n",
    "    \n",
    "    if y is None:\n",
    "        return y_pred\n",
    "    \n",
    "    eval_acc = np.mean(y_pred == y)\n",
    "    \n",
    "    return y_pred, eval_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params, hist = train_two_layer_perceptron(train_data, train_labels,\n",
    "                                          val_data, val_labels,\n",
    "                                          hidden_size=128, batch_size=100,\n",
    "                                          num_epochs=10, learning_rate=1e-4,\n",
    "                                          learning_rate_decay=0.95, reg_factor=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 1, figsize=(10,10))\n",
    "ax[0].plot(hist['loss'])\n",
    "ax[0].set_title('Training loss') \n",
    "ax[0].set_xlabel('number of iterations')\n",
    "ax[0].set_ylabel('loss')\n",
    "\n",
    "ax[1].plot(hist['train_acc'])\n",
    "ax[1].set_title('Training accuracy') \n",
    "ax[1].set_xlabel('number of iterations')\n",
    "ax[1].set_ylabel('accuracy')\n",
    "\n",
    "ax[2].plot(hist['val_acc'])\n",
    "ax[2].set_title('Validation accuracy') \n",
    "ax[2].set_xlabel('number of iterations')\n",
    "ax[2].set_ylabel('accuracy')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vis_utils import visualize_grid\n",
    "\n",
    "W1 = params['W1']\n",
    "W1 = np.reshape(W1, (-1,28,28))\n",
    "\n",
    "fig = plt.figure(figsize=(15,15))\n",
    "plt.imshow(visualize_grid(W1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels, test_acc = predict_two_layer_perceptron(params, test_data, test_labels)\n",
    "print('test accuracy:', test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
